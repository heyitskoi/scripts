#!/usr/bin/env bash
# ============================================================================
# SUPERSETUP v2 — first Ops box bootstrap (Ubuntu/Debian)
# Fast track: Docker, observability (Prom/Grafana/Loki/Alertmanager), Portainer,
# local registry, SSH CA, Uptime-Kuma, and an Ansible project to manage it.
# Sarcastic comments, minimal ceremony, actually idempotent.
# ============================================================================

set -euo pipefail
IFS=$'\n\t'
[ "$(id -u)" -eq 0 ] && SUDO="" || SUDO="sudo"
LOG_FILE=/var/log/supersetup.log # standard location for easy rotation
$SUDO touch "$LOG_FILE" && $SUDO chmod 0600 "$LOG_FILE" && $SUDO chown root:root "$LOG_FILE"
ME="${OPS_OWNER:-${SUDO_USER:-${LOGNAME:-$USER}}}"

# Avoid interactive dpkg/apt prompts (e.g. tzdata)
export DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive}

say(){
  printf "\033[1;32m[+]\033[0m %s\n" "$*"
  printf "[+] %s\n" "$*" | ${SUDO:-} tee -a "$LOG_FILE" >/dev/null
}
nope(){
  printf "\033[1;31m[!]\033[0m %s\n" "$*" >&2
  printf "[!] %s\n" "$*" | ${SUDO:-} tee -a "$LOG_FILE" >/dev/null
  exit 1
}

trap 'nope "Failed at line $LINENO. See $LOG_FILE for details."' ERR

LOCK_FILE=${LOCK_FILE:-/var/lock/supersetup.lock}
if [ -n "$SUDO" ]; then
  $SUDO install -o "$ME" -g "$ME" -m 0644 /dev/null "$LOCK_FILE" 2>/dev/null || {
    LOCK_FILE=/run/lock/supersetup.lock
    $SUDO install -o "$ME" -g "$ME" -m 0644 /dev/null "$LOCK_FILE" 2>/dev/null || true
  }
fi
exec 9>>"$LOCK_FILE" || nope "Cannot open lock file $LOCK_FILE"
flock -n 9 || nope "Another run is in progress."

# ------------------------------------------------------------
# Version variables (override via environment or versions file)
# ------------------------------------------------------------
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VERSION_FILE="${VERSION_FILE:-$SCRIPT_DIR/versions.env}"
# shellcheck source=/dev/null
[ -f "$VERSION_FILE" ] && . "$VERSION_FILE"

# Provide sane defaults; environment or versions file can override
TERRAFORM_VERSION="${TERRAFORM_VERSION:-1.9.5}"
PACKER_VERSION="${PACKER_VERSION:-1.11.2}"
TRIVY_VERSION="${TRIVY_VERSION:-0.54.1}"
PROMETHEUS_VERSION="${PROMETHEUS_VERSION:-v2.55.0}"
ALERTMANAGER_VERSION="${ALERTMANAGER_VERSION:-v0.27.0}"
GRAFANA_VERSION="${GRAFANA_VERSION:-10.4.6}"
LOKI_VERSION="${LOKI_VERSION:-2.9.8}"
PROMTAIL_VERSION="${PROMTAIL_VERSION:-2.9.8}"
PORTAINER_VERSION="${PORTAINER_VERSION:-2.21.4}"
REGISTRY_VERSION="${REGISTRY_VERSION:-2}"
UPTIME_KUMA_VERSION="${UPTIME_KUMA_VERSION:-1}"

# ------------------------------------------------------------
# CLI modes: run (default), dry-run, health
# ------------------------------------------------------------
MODE="${1:-run}"   # run | dry-run | health
case "$MODE" in
  run) : ;;
  dry-run) export DRY_RUN=1 ;;
  health) export HEALTH_ONLY=1 ;;
  *) nope "Unknown mode: $MODE (use: run|dry-run|health)";;
esac

# ------------------------------------------------------------
# Sanity checks
# ------------------------------------------------------------
say "Ops owner user: ${ME}"
if [ -n "$SUDO" ] && ! sudo -n true 2>/dev/null; then
  nope "Passwordless sudo required for non-root runs. Configure NOPASSWD or run the script with sudo."
fi
# Ensure systemd is PID 1
[ -d /run/systemd/system ] || nope "systemd is required (PID 1)."
# Source OS info; assumes systemd as PID 1
. /etc/os-release || nope "Unknown OS."
if [[ ! "${ID}" =~ (ubuntu|debian) ]] && [[ ! "${ID_LIKE:-}" =~ (ubuntu|debian) ]]; then
  nope "Use Ubuntu/Debian for this bootstrap."
fi

# Retry wrapper for apt-get commands
apt_retry(){ # args: apt-get subcommand + options
  local n=0; until [ $n -ge 3 ]; do
    $SUDO apt-get "$@" && break
    n=$((n+1)); sleep 3
    say "apt-get retry $n…"
  done
  [ $n -lt 3 ] || nope "apt-get failed after retries: $*"
}
DRY_RUN_ONLY=0
if [ "${MODE:-run}" = "dry-run" ]; then
  say "DRY RUN: skip package installs; still scaffold + run Ansible --check."
  DRY_RUN_ONLY=1
fi

# ------------------------------------------------------------
# Base packages
# ------------------------------------------------------------
if [ "$DRY_RUN_ONLY" -eq 0 ]; then
  say "Updating packages…"
  apt_retry update
  # Optional but useful to keep the base system fresh without prompts
  apt_retry -o Dpkg::Options::="--force-confnew" -y dist-upgrade || true
  apt_retry install -y --no-install-recommends \
    ca-certificates gnupg lsb-release curl git unzip tar make jq \
    ufw fail2ban python3 python3-pip nmap
fi

# ------------------------------------------------------------
# Docker CE (official repo)
# ------------------------------------------------------------
install_docker(){
  if command -v docker >/dev/null 2>&1; then
    say "Docker already present. Enabling service."
    $SUDO systemctl enable --now docker || true
    return 0
  fi
  say "Installing Docker CE…"
  # Remove conflicting packages quietly
  $SUDO apt-get -y remove docker.io containerd runc || true
  $SUDO apt-get -y purge  docker.io containerd runc || true

  $SUDO install -m 0755 -d /etc/apt/keyrings
  [ -f /etc/apt/keyrings/docker.gpg ] || \
    curl -fsSL "https://download.docker.com/linux/${ID}/gpg" | $SUDO gpg --dearmor -o /etc/apt/keyrings/docker.gpg

  if [ ! -f /etc/apt/sources.list.d/docker.list ]; then
    arch=$(dpkg --print-architecture)
    codename=$(. /etc/os-release && echo "$VERSION_CODENAME")
    echo "deb [arch=${arch} signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/${ID} ${codename} stable" | \
      $SUDO tee /etc/apt/sources.list.d/docker.list >/dev/null
  fi
  apt_retry update
  apt_retry install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  $SUDO systemctl enable --now docker
}
if [ "$DRY_RUN_ONLY" -eq 0 ]; then
  install_docker
  if [ "$ME" != "root" ]; then
    $SUDO usermod -aG docker "$ME" || true
  fi
fi

# ------------------------------------------------------------
# Helpers to install single-file archives
# ------------------------------------------------------------
install_zip_bin(){ # name ver url sha256
  local name="$1" ver="$2" url="$3" sha="$4"
  if command -v "$name" >/dev/null 2>&1 && "$name" version 2>/dev/null | grep -q "$ver"; then
    return 0
  fi
  say "Installing $name $ver…"
  local tmp
  tmp=$(mktemp "/tmp/${name}.XXXXXX.zip")
  curl -fsSL -o "$tmp" "$url"
  local sum
  sum=$(sha256sum "$tmp" | awk '{print $1}')
  if [ "$sum" != "$sha" ]; then
    rm -f "$tmp"
    nope "$name checksum mismatch"
  fi
  $SUDO unzip -o "$tmp" -d /usr/local/bin/
  $SUDO chmod +x "/usr/local/bin/${name}"
  $SUDO mv "/usr/local/bin/${name}" "/usr/local/bin/${name}-${ver}"
  $SUDO ln -sfn "/usr/local/bin/${name}-${ver}" "/usr/local/bin/${name}"
  rm -f "$tmp"
}
install_tgz_bin(){ # name ver url sha256
  local name="$1" ver="$2" url="$3" sha="$4"
  if command -v "$name" >/dev/null 2>&1 && "$name" version 2>/dev/null | grep -q "$ver"; then
    return 0
  fi
  say "Installing $name $ver…"
  local tmp tmpdir sum bin
  tmp=$(mktemp "/tmp/${name}.XXXXXX.tgz")
  curl -fsSL -o "$tmp" "$url"
  sum=$(sha256sum "$tmp" | awk '{print $1}')
  if [ "$sum" != "$sha" ]; then
    rm -f "$tmp"
    nope "$name checksum mismatch"
  fi
  tmpdir=$(mktemp -d "/tmp/${name}-XXXXXX")
  tar -xzf "$tmp" -C "$tmpdir"
  bin=$(find "$tmpdir" -type f -name "$name" | head -n1 || true)
  if [ -z "$bin" ]; then
    rm -f "$tmp"
    rm -rf "$tmpdir"
    nope "Failed to locate $name in archive"
  fi
  $SUDO install -m0755 "$bin" "/usr/local/bin/${name}-${ver}"
  $SUDO ln -sfn "/usr/local/bin/${name}-${ver}" "/usr/local/bin/${name}"
  rm -f "$tmp"
  rm -rf "$tmpdir"
}

import_hashicorp_release_key() {
  GNUPGHOME="$(mktemp -d)"; export GNUPGHOME
  curl -fsSL https://www.hashicorp.com/.well-known/pgp-key.txt | gpg --import
  expected="C874011F0AB405110D02105534365D9472D7468F"
  gpg --fingerprint --with-colons "HashiCorp Security" | grep -q "$expected" \
    || { rm -rf "$GNUPGHOME"; nope "HashiCorp PGP fingerprint mismatch"; }
}

verify_hashicorp_sums() { # name ver -> echos path to sums file
  local name="$1" ver="$2" base="https://releases.hashicorp.com/${name}/${ver}"
  local sums="/tmp/${name}_${ver}_SHA256SUMS"
  curl -fsSLo "${sums}"     "${base}/${name}_${ver}_SHA256SUMS"
  curl -fsSLo "${sums}.sig" "${base}/${name}_${ver}_SHA256SUMS.sig"
  gpg --verify "${sums}.sig" "${sums}" >/dev/null 2>&1 \
    || { rm -rf "$GNUPGHOME"; nope "Invalid ${name} SHASUMS signature"; }
  echo "${sums}"
}

install_zip_bin_hc(){ # name ver arch
  local name="$1" ver="$2" arch="$3" sums url tmp want got
  command -v "$name" >/dev/null 2>&1 && "$name" version 2>/dev/null | grep -q "$ver" && return 0
  say "Installing $name $ver…"
  url="https://releases.hashicorp.com/${name}/${ver}/${name}_${ver}_linux_${arch}.zip"
  import_hashicorp_release_key
  sums="$(verify_hashicorp_sums "$name" "$ver")"
  want="$(grep " ${name}_${ver}_linux_${arch}.zip" "$sums" | awk '{print $1}')"
  tmp="$(mktemp "/tmp/${name}.XXXXXX.zip")"
  curl -fsSLo "$tmp" "$url"
  got="$(sha256sum "$tmp" | awk '{print $1}')"
  [ "$got" = "$want" ] || { rm -f "$tmp"; rm -rf "$GNUPGHOME"; nope "$name checksum mismatch"; }
  $SUDO unzip -o "$tmp" -d /usr/local/bin/
  $SUDO chmod +x "/usr/local/bin/${name}"
  $SUDO mv "/usr/local/bin/${name}" "/usr/local/bin/${name}-${ver}"
  $SUDO ln -sfn "/usr/local/bin/${name}-${ver}" "/usr/local/bin/${name}"
  rm -f "$tmp" "$sums" "${sums}.sig"
  rm -rf "$GNUPGHOME"
}

# Arch detect
ARCH="$(uname -m)"
case "$ARCH" in
  x86_64) ARCH=amd64 ;;
  aarch64|arm64) ARCH=arm64 ;;
  armv7l) ARCH=arm ;;
  *) say "Unknown arch $ARCH; URLs may need adjusting." ;;
esac
[ "$DRY_RUN_ONLY" -eq 0 ] && install_zip_bin_hc terraform "$TERRAFORM_VERSION" "$ARCH"
[ "$DRY_RUN_ONLY" -eq 0 ] && install_zip_bin_hc packer "$PACKER_VERSION" "$ARCH"

# Trivy (prebuilt for amd64 and arm64)
if [ "$DRY_RUN_ONLY" -eq 0 ]; then
  if [ "$ARCH" = "amd64" ]; then
    trivy_url="https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_Linux-64bit.tar.gz"
    trivy_sha="$(curl -fsSL "https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_checksums.txt" \
      | grep "trivy_${TRIVY_VERSION}_Linux-64bit.tar.gz" | awk '{print $1}')"
    install_tgz_bin trivy "$TRIVY_VERSION" "$trivy_url" "$trivy_sha" || true
  elif [ "$ARCH" = "arm64" ]; then
    trivy_url="https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_Linux-ARM64.tar.gz"
    trivy_sha="$(curl -fsSL "https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_checksums.txt" \
      | grep "trivy_${TRIVY_VERSION}_Linux-ARM64.tar.gz" | awk '{print $1}')"
    install_tgz_bin trivy "$TRIVY_VERSION" "$trivy_url" "$trivy_sha" || true
  else
    say "Skipping trivy prebuilt for ARCH=$ARCH; install manually later."
  fi
fi

# ------------------------------------------------------------
# Ansible
# ------------------------------------------------------------
if [ "$DRY_RUN_ONLY" -eq 0 ]; then
  say "Installing Ansible…"
  apt_retry install -y ansible
fi

# ------------------------------------------------------------
# Scaffold the ops project — inventories, vars, templates, playbook, compose
# ------------------------------------------------------------
say "Laying down /srv/ops/ansible…"
$SUDO mkdir -p /srv/ops/ansible/templates
# ownership fix: use $ME (not ${USER} which can be root)
$SUDO chown -R "$ME":"$ME" /srv/ops
cd /srv/ops/ansible

# -------- inventory --------
cat > inventory.ini <<'INI'
[ops]
localhost ansible_connection=local
INI

# -------- group vars --------
mkdir -p group_vars
cat > group_vars/all.yml <<'YAML'
# Optional front-door hostnames (for proxy/TLS later)
ops_domain: ""  # e.g., ops.example.com
ops_listen_ip: "127.0.0.1"      # change to your LAN/VPN IP to limit exposure
slack_webhook_url: ""           # set for Alertmanager
telegram_bot_token: ""
telegram_chat_id: ""

enable_wireguard: false     # Set to true to install WireGuard VPN
enable_crowdsec: false      # Set to true to install CrowdSec IDS
disable_ipv6: false         # Set true to disable IPv6 and enforce IPv4-only

# WireGuard defaults (adjust to your environment)
wireguard_address: "10.0.0.1/24"
wireguard_listen_port: 51820
wireguard_private_key: "<changeme>"
wireguard_peers: []

# CrowdSec log sources
crowdsec_log_files:
  - /var/log/auth.log

grafana_admin_password: "changeme"  # auto-generated if unset (use Ansible Vault to override)
portainer_admin_password_hash: ""   # bcrypt hash; auto-generated if empty
grafana_digest: ""          # e.g., sha256:abc...
prometheus_digest: ""
alertmanager_digest: ""
loki_digest: ""
promtail_digest: ""
portainer_digest: ""

# Multinode-curious knobs (safe defaults for single host)
prom_host: "localhost"
grafana_host: "localhost"

prometheus_url: "http://{{ prom_host }}:9090"

# Loki endpoints
loki_url_container: "http://loki:3100"        # for Docker services talking to Loki
loki_url_agents: "http://{{ ansible_default_ipv4.address }}:3100"  # for remote hosts

# Prometheus targets
prom_targets: []  # leave empty; the playbook already auto-adds this host’s :9100
YAML

# -------- templates --------
# Prometheus
cat > templates/prometheus.yml.j2 <<'J2'
global:
  scrape_interval: 15s
rule_files:
  - /etc/prometheus/alert_rules.yml
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  - job_name: 'nodes'
    static_configs:
      - targets: {{ ((prom_targets | default([])) + [ansible_default_ipv4.address ~ ':9100']) | unique | to_json }}
  - job_name: 'grafana'
    static_configs:
      - targets: ['grafana:3000']
J2

# Alertmanager (Slack optional)
cat > templates/alertmanager.yml.j2 <<'J2'
route:
  receiver: default
{% if slack_webhook_url|length > 0 %}
  routes:
    - matchers: []
      receiver: slack
      continue: false
{% endif %}
  group_by: ['alertname']
  group_wait: 30s
  repeat_interval: 3h
receivers:
  - name: default
{% if slack_webhook_url|length > 0 %}
  - name: slack
    slack_configs:
      - api_url: "{{ slack_webhook_url }}"
        send_resolved: true
        channel: "#alerts"
        title: |
          {{ '{{' }} .CommonAnnotations.summary {{ '}}' }}
        text: |
          {{ '{{' }} range .Alerts {{ '}}' }}*{{ '{{' }} .Annotations.summary {{ '}}' }}* on {{ '{{' }} .Labels.instance {{ '}}' }}{{ '{{' }} end {{ '}}' }}
{% endif %}
J2

# Prometheus alert rules (raw to preserve Go templating)
cat > templates/alert_rules.yml.j2 <<'J2'
{% raw %}
groups:
  - name: basic-host
    rules:
      - alert: HostDown
        expr: up == 0
        for: 2m
        labels: { severity: critical }
        annotations: { summary: "Instance {{ $labels.instance }} is down" }
      - alert: HostDiskAlmostFull
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.10
        for: 5m
        labels: { severity: warning }
        annotations: { summary: "Root disk on {{ $labels.instance }} is almost full" }
      - alert: PrometheusDiskSpaceLow
        expr: prometheus_tsdb_low_storage_space == 1
        for: 5m
        labels: { severity: warning }
        annotations: { summary: "Prometheus TSDB storage is almost full" }
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels: { severity: critical }
        annotations: { summary: "Grafana is down" }
{% endraw %}
J2

# Promtail (points to this box's Loki)
cat > templates/promtail-config.yml.j2 <<'J2'
server:
  http_listen_port: 9080
clients:
  - url: "{{ loki_url_container }}/loki/api/v1/push"
positions:
  filename: /var/lib/promtail/positions.yaml
scrape_configs:
  - job_name: system
    static_configs:
      - targets: [localhost]
        labels:
          job: varlogs
          __path__: /var/log/**/*.log
  - job_name: journal
    journal:
      path: /var/log/journal
      labels:
        job: systemd-journal
J2

# Agent installer (for remote hosts)
cat > templates/install-agents.sh.j2 <<'J2'
#!/usr/bin/env bash
set -euo pipefail
export DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive}
if command -v apt-get >/dev/null; then
  apt_retry(){ # args: apt-get subcommand + options
    local n=0; until [ $n -ge 3 ]; do
      sudo apt-get "$@" && break
      n=$((n+1)); sleep 3
      echo "apt-get retry $n…" >&2
    done
    [ $n -lt 3 ]
  }
  apt_retry update
  apt_retry install -y prometheus-node-exporter
  sudo mkdir -p /etc/systemd/system/prometheus-node-exporter.service.d
  cat <<'EOF' | sudo tee /etc/systemd/system/prometheus-node-exporter.service.d/override.conf >/dev/null
[Service]
ExecStart=
ExecStart=/usr/bin/prometheus-node-exporter --web.listen-address=127.0.0.1:9100
EOF
  sudo systemctl daemon-reload
  sudo systemctl enable --now prometheus-node-exporter
fi
sudo mkdir -p /etc/promtail /var/lib/promtail
cat <<CFG | sudo tee /etc/promtail/config.yml >/dev/null
server: { http_listen_port: 9080 }
clients: [ { url: "{{ loki_url_agents }}/loki/api/v1/push" } ]
positions: { filename: /var/lib/promtail/positions.yaml }
scrape_configs:
  - job_name: varlogs
    static_configs:
      - targets: [localhost]
        labels:
          job: varlogs
          __path__: /var/log/**/*.log
CFG
# Run promtail via Docker for simplicity
sudo docker run -d --name promtail --restart unless-stopped \
  -v /etc/promtail/config.yml:/etc/promtail/config.yml:ro \
  -v /var/log:/var/log:ro \
  -v /var/lib/promtail:/var/lib/promtail \
  -p 9080:9080 \
  grafana/promtail:__PROMTAIL_VERSION__ -config.file=/etc/promtail/config.yml
J2
sed -i "s/__PROMTAIL_VERSION__/${PROMTAIL_VERSION}/g" templates/install-agents.sh.j2

# WireGuard interface config
cat > templates/wg0.conf.j2 <<'J2'
[Interface]
Address = {{ wireguard_address }}
ListenPort = {{ wireguard_listen_port }}
PrivateKey = {{ wireguard_private_key }}
{% for peer in wireguard_peers %}
[Peer]
PublicKey = {{ peer.public_key }}
AllowedIPs = {{ peer.allowed_ips }}
{% if peer.endpoint is defined %}Endpoint = {{ peer.endpoint }}{% endif %}
{% endfor %}
J2

# CrowdSec acquisition config
cat > templates/crowdsec-acquis.yaml.j2 <<'J2'
filenames:
{% for file in crowdsec_log_files %}
  - {{ file }}
{% endfor %}
J2

# -------- site playbook --------
cat > site.yml <<'YAML'
---
- hosts: ops
  become: true
  vars_files:
    - group_vars/all.yml

  pre_tasks:
    - name: Gather facts
      setup:
    - name: Collect service facts
      service_facts:
    - name: Ensure persistent systemd-journald storage
      file: { path: /var/log/journal, state: directory, mode: "0755" }
    - name: Restart systemd-journald to use persistent storage
      service: { name: systemd-journald, state: restarted }

  tasks:
    - name: Create ops directories
      file:
        path: "{{ item }}"
        state: directory
        recurse: yes
        mode: "0755"
      loop:
        - /srv/ops/compose
        - /srv/ops/config
        - /srv/ops/installers
        - /srv/ops/.docker
        - /srv/ops/data/prometheus
        - /srv/ops/data/grafana
        - /srv/ops/data/loki
        - /srv/ops/data/registry
        - /srv/ops/data/uptime-kuma
        - /opt/ssh-ca

    - name: Generate OpenSSH CA (once)
      command: ssh-keygen -t ed25519 -f /opt/ssh-ca/ssh_ca -N "" -C "ops-ssh-ca"
      args: { creates: /opt/ssh-ca/ssh_ca }

    - name: Publish TrustedUserCAKeys
      copy:
        src: /opt/ssh-ca/ssh_ca.pub
        dest: /etc/ssh/trusted-user-ca-keys.pem
        owner: root
        group: root
        mode: "0644"

    - name: Ensure SSH revoked keys file exists
      copy:
        dest: /etc/ssh/revoked_keys
        owner: root
        group: root
        mode: "0644"
        content: ""
        force: false

    - name: Reference revoked keys in sshd
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^RevokedKeys'
        line: 'RevokedKeys /etc/ssh/revoked_keys'
        create: yes
        backup: yes
      notify: reload ssh

    - name: Enforce CA trust in sshd
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^TrustedUserCAKeys'
        line: 'TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem'
        create: yes
        backup: yes
      notify: reload ssh

    - name: SSH hardening (no root pwd, sane auth)
      blockinfile:
        path: /etc/ssh/sshd_config
        create: yes
        block: |
          PasswordAuthentication no
          PermitRootLogin prohibit-password
          PubkeyAuthentication yes
          ChallengeResponseAuthentication no
      notify: reload ssh

    - name: Signer helper
      copy:
        dest: /usr/local/bin/ssh-sign
        mode: "0755"
        content: |
          #!/usr/bin/env bash
          # ssh-sign <user_pubkey_file> <principals> <hours_valid>
          set -euo pipefail
          PUB="${1:-}"; PRIN="${2:-$USER}"; HRS="${3:-24}"
          [[ -f "$PUB" ]] || { echo "Need a public key file"; exit 1; }
          ssh-keygen -s /opt/ssh-ca/ssh_ca -I "$PRIN" -n "$PRIN" -V +"${HRS}"h "$PUB"
          echo "Signed cert: ${PUB}-cert.pub (valid ${HRS}h for ${PRIN})"

    # -------- UFW: safe sequence --------
    - name: UFW allow SSH before enabling
      ufw:
        rule: allow
        port: "22"

    - name: Set UFW default incoming policy to deny
      ufw:
        state: disabled
        direction: incoming
        policy: deny
    - name: Set UFW default outgoing policy to allow
      ufw:
        state: disabled
        direction: outgoing
        policy: allow

    - name: Disable IPv6 when requested
      block:
        - name: Disable IPv6 via sysctl
          copy:
            dest: /etc/sysctl.d/90-no-ipv6.conf
            mode: "0644"
            content: |
              net.ipv6.conf.all.disable_ipv6=1
              net.ipv6.conf.default.disable_ipv6=1
        - name: Apply sysctl changes
          command: sysctl --system
        - name: UFW IPv4 only
          lineinfile:
            path: /etc/default/ufw
            regexp: '^IPV6='
            line: 'IPV6=no'
      when: disable_ipv6

    - name: Allow required ports
      ufw:
        rule: allow
        port: "{{ item }}"
      loop:
        - "22"    # SSH
      notify: enable ufw

    - name: Allow ops stack ports when not on loopback
      ufw:
        rule: allow
        port: "{{ item }}"
      loop: ["3000","9090","9093","3100","9000","3001"]
      when: ops_listen_ip not in ['127.0.0.1','::1']
      notify: enable ufw

    - name: Allow node exporter from Docker networks
      ufw: { rule: allow, port: "9100", src: "172.16.0.0/12" }

    - name: Allow WireGuard port
      ufw:
        rule: allow
        port: "{{ wireguard_listen_port }}"
        proto: udp
      when: enable_wireguard

    - name: Fail2ban config (basic ssh)
      copy:
        dest: /etc/fail2ban/jail.d/ssh.local
        content: |
          [sshd]
          enabled = true
          maxretry = 6
          bantime = 1h
          findtime = 10m
      notify: restart fail2ban

    # -------- Optional WireGuard VPN --------
    - name: Install WireGuard
      apt:
        name: wireguard
        state: present
        update_cache: yes
      when: enable_wireguard

    - name: Validate WireGuard key placeholder
      fail:
        msg: "Set a real wireguard_private_key in group_vars/all.yml"
      when: enable_wireguard and (wireguard_private_key is match('^<changeme>'))

    - name: WireGuard config
      template:
        src: templates/wg0.conf.j2
        dest: /etc/wireguard/wg0.conf
        mode: "0600"
      when: enable_wireguard
      notify: restart wireguard

    - name: Enable WireGuard service
      systemd:
        name: wg-quick@wg0
        enabled: true
        state: started
      when: enable_wireguard

    # -------- Optional CrowdSec IDS --------
    - name: Install CrowdSec
      apt:
        name:
          - crowdsec
          - crowdsec-firewall-bouncer-iptables
        state: present
        update_cache: yes
      when: enable_crowdsec

    - name: CrowdSec acquisition config
      template:
        src: templates/crowdsec-acquis.yaml.j2
        dest: /etc/crowdsec/acquis.yaml
        mode: "0644"
      when: enable_crowdsec
      notify: restart crowdsec

    - name: Ensure CrowdSec service
      service: { name: crowdsec, state: started, enabled: true }
      when: enable_crowdsec

    - name: Ensure CrowdSec firewall bouncer
      service: { name: crowdsec-firewall-bouncer, state: started, enabled: true }
      when: enable_crowdsec

    # -------- Docker daemon.json --------
    - name: Configure Docker log rotation + BuildKit
      copy:
        dest: /etc/docker/daemon.json
        mode: "0644"
        content: |
          {
            "log-driver": "json-file",
            "log-opts": { "max-size": "10m", "max-file": "5" },
            "features": { "buildkit": true },
            "live-restore": true
          }
      notify: restart docker

    - name: Ensure node exporter installed
      apt:
        name: prometheus-node-exporter
        state: present
        update_cache: yes
    
    - name: Ensure node exporter override dir
      file:
        path: /etc/systemd/system/prometheus-node-exporter.service.d
        state: directory
        mode: "0755"

    - name: Bind node exporter on all interfaces (restricted by UFW)
      copy:
        dest: /etc/systemd/system/prometheus-node-exporter.service.d/override.conf
        mode: "0644"
        content: |
          [Service]
          ExecStart=
          ExecStart=/usr/bin/prometheus-node-exporter --web.listen-address=:9100
      notify: [reload systemd, restart node exporter]

    - name: Enable node exporter service
      service: { name: prometheus-node-exporter, state: started, enabled: true }

    - name: Prometheus config
      template:
        src: templates/prometheus.yml.j2
        dest: /srv/ops/config/prometheus.yml
        mode: "0644"

    - name: Alertmanager config
      template:
        src: templates/alertmanager.yml.j2
        dest: /srv/ops/config/alertmanager.yml
        mode: "0644"
    - name: Prometheus alert rules (template; preserve Go templates)
      template:
        src: templates/alert_rules.yml.j2
        dest: /srv/ops/config/alert_rules.yml
        mode: "0644"

    - name: Loki config
      copy:
        dest: /srv/ops/config/loki-config.yml
        mode: "0644"
        content: |
          auth_enabled: false
          server: { http_listen_port: 3100 }
          common:
            path_prefix: /loki
          storage_config:
            filesystem: { }
          schema_config:
            configs:
              - from: 2020-10-24
                store: boltdb-shipper
                object_store: filesystem
                schema: v11
                index:
                  prefix: index_
                  period: 24h

    # -------- Promtail state dir --------
    - name: Ensure promtail state dir exists
      file:
        path: /var/lib/promtail
        state: directory
        mode: "0755"

    - name: Promtail config (this host)
      template:
        src: templates/promtail-config.yml.j2
        dest: /srv/ops/config/promtail-config.yml
        mode: "0644"

    # -------- Grafana provisioning (no click-ops) --------
    - name: Grafana provisioning dirs
      file:
        path: /srv/ops/config/grafana/provisioning/datasources
        state: directory
        mode: "0755"

    - name: Grafana default datasources
      copy:
        dest: /srv/ops/config/grafana/provisioning/datasources/datasources.yml
        mode: "0644"
        content: |
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://prometheus:9090
              isDefault: true
            - name: Loki
              type: loki
              access: proxy
              url: http://loki:3100

    - name: Secrets dir
      file:
        path: /srv/ops/config/secrets
        state: directory
        mode: "0700"
    - name: Generate Grafana admin password if unset
      command: openssl rand -base64 24
      register: grafana_pw
      when: grafana_admin_password in ['', 'changeme']

    - name: Persist generated Grafana password
      lineinfile:
        path: /srv/ops/ansible/group_vars/all.yml
        regexp: '^grafana_admin_password:'
        line: 'grafana_admin_password: "{{ grafana_pw.stdout }}"'
      when: grafana_pw is defined

    - name: Set grafana_admin_password fact
      set_fact:
        grafana_admin_password: "{{ grafana_pw.stdout }}"
      when: grafana_pw is defined

    - name: Generate Portainer admin password if unset
      command: openssl rand -base64 24
      register: portainer_pw_plain
      when: portainer_admin_password_hash | length == 0

    - name: Hash Portainer admin password
      command: "openssl passwd -bcrypt {{ portainer_pw_plain.stdout }}"
      register: portainer_pw_hash
      when: portainer_pw_plain is defined

    - name: Persist generated Portainer hash
      lineinfile:
        path: /srv/ops/ansible/group_vars/all.yml
        regexp: '^portainer_admin_password_hash:'
        line: 'portainer_admin_password_hash: "{{ portainer_pw_hash.stdout }}"'
      when: portainer_pw_hash is defined

    - name: Set portainer_admin_password_hash fact
      set_fact:
        portainer_admin_password_hash: "{{ portainer_pw_hash.stdout }}"
      when: portainer_pw_hash is defined

    - name: Grafana admin password secret
      copy:
        dest: /srv/ops/config/secrets/grafana_admin
        mode: "0600"
        content: "{{ grafana_admin_password }}"

    - name: Portainer admin password hash secret
      copy:
        dest: /srv/ops/config/secrets/portainer_admin
        mode: "0600"
        content: "{{ portainer_admin_password_hash }}"

    - name: Save Portainer admin password once
      copy:
        dest: /srv/ops/config/secrets/PORTAINER_ADMIN_PASSWORD.once
        mode: "0600"
        content: "{{ portainer_pw_plain.stdout }}"
      when: portainer_pw_plain is defined and (not ansible_check_mode|default(false))

    - name: Display Portainer admin password
      debug:
        msg: "Portainer admin password stored in /srv/ops/config/secrets/PORTAINER_ADMIN_PASSWORD.once (delete after reading): {{ portainer_pw_plain.stdout }}"
      when: portainer_pw_plain is defined and (not ansible_check_mode|default(false))

    - name: Compose file for ops stack
      copy:
        dest: /srv/ops/compose/ops.yml
        mode: "0644"
        content: |
          services:
            # For internet exposure: put behind a reverse proxy (Traefik/Nginx) with TLS.
            # These ports are LAN-facing convenience only.
            prometheus:
              image: "{{ 'prom/prometheus@' ~ prometheus_digest if prometheus_digest|length > 0 else 'prom/prometheus:__PROMETHEUS_VERSION__' }}"
              command: ["--config.file=/etc/prometheus/prometheus.yml","--web.enable-lifecycle","--web.route-prefix=/","--storage.tsdb.retention.time=15d","--storage.tsdb.retention.size=8GB"]
              volumes:
                - /srv/ops/config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
                - /srv/ops/config/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
                - prometheus:/prometheus
                # Bind-mount alternative (pick one, then remove named volume "prometheus" below):
                # - /srv/ops/data/prometheus:/prometheus
              ports: ["{{ ops_listen_ip }}:9090:9090"]
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "(which curl && curl -fsS http://localhost:9090/-/ready) || (which wget && wget -qO- http://localhost:9090/-/ready)"]
                interval: 10s
                timeout: 3s
                retries: 10
            alertmanager:
              image: "{{ 'prom/alertmanager@' ~ alertmanager_digest if alertmanager_digest|length > 0 else 'prom/alertmanager:__ALERTMANAGER_VERSION__' }}"
              volumes:
                - /srv/ops/config/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
              ports: ["{{ ops_listen_ip }}:9093:9093"]
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "(which curl && curl -fsS http://localhost:9093/-/ready) || (which wget && wget -qO- http://localhost:9093/-/ready)"]
                interval: 10s
                timeout: 3s
                retries: 10
            grafana:
              image: "{{ 'grafana/grafana@' ~ grafana_digest if grafana_digest|length > 0 else 'grafana/grafana:__GRAFANA_VERSION__' }}"
              environment:
                - GF_SECURITY_ADMIN_USER=admin
                - GF_SECURITY_ADMIN_PASSWORD__FILE=/run/secrets/grafana_admin
              secrets:
                - grafana_admin
              volumes:
                - grafana:/var/lib/grafana
                - /srv/ops/config/grafana/provisioning:/etc/grafana/provisioning:ro
              ports: ["{{ ops_listen_ip }}:3000:3000"]
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "(which curl && curl -fsS http://localhost:3000/api/health) || (which wget && wget -qO- http://localhost:3000/api/health)"]
                interval: 10s
                timeout: 3s
                retries: 10
            loki:
              image: "{{ 'grafana/loki@' ~ loki_digest if loki_digest|length > 0 else 'grafana/loki:__LOKI_VERSION__' }}"
              command: ["-config.file=/etc/loki/loki-config.yml"]
              volumes:
                - /srv/ops/config/loki-config.yml:/etc/loki/loki-config.yml:ro
                - loki:/loki
              ports: ["{{ ops_listen_ip }}:3100:3100"]
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "(which curl && curl -fsS http://localhost:3100/ready) || (which wget && wget -qO- http://localhost:3100/ready)"]
                interval: 10s
                timeout: 3s
                retries: 10
            promtail:
              image: "{{ 'grafana/promtail@' ~ promtail_digest if promtail_digest|length > 0 else 'grafana/promtail:__PROMTAIL_VERSION__' }}"
              command: ["-config.file=/etc/promtail/config.yml"]
              volumes:
                - /srv/ops/config/promtail-config.yml:/etc/promtail/config.yml:ro
                - /var/log/journal:/var/log/journal:ro
                - /var/log:/var/log:ro
                - /var/lib/promtail:/var/lib/promtail
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "(which curl && curl -fsS http://localhost:9080/ready) || (which wget && wget -qO- http://localhost:9080/ready)"]
                interval: 10s
                timeout: 3s
                retries: 10
            portainer:
              image: "{{ 'portainer/portainer-ce@' ~ portainer_digest if portainer_digest|length > 0 else 'portainer/portainer-ce:__PORTAINER_VERSION__' }}"
              volumes:
                - /var/run/docker.sock:/var/run/docker.sock
                - portainer:/data
              ports: ["{{ ops_listen_ip }}:9000:9000"]
              command: ["--admin-password-file", "/run/secrets/portainer_admin"]
              secrets:
                - portainer_admin
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "(which curl && curl -fsS http://localhost:9000/api/status) || (which wget && wget -qO- http://localhost:9000/api/status)"]
                interval: 10s
                timeout: 3s
                retries: 10
            # Local development registry; no auth/TLS. Do not expose beyond localhost.
            registry:
              image: registry:__REGISTRY_VERSION__
              environment:
                REGISTRY_STORAGE_DELETE_ENABLED: "true"
              volumes:
                - registry:/var/lib/registry
              ports: ["127.0.0.1:5000:5000"]  # bound to loopback; do not expose
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "(which curl && curl -fsS http://localhost:5000/v2/) || (which wget && wget -qO- http://localhost:5000/v2/)"]
                interval: 10s
                timeout: 3s
                retries: 10
            uptime-kuma:
              image: louislam/uptime-kuma:__UPTIME_KUMA_VERSION__
              volumes:
                - uptimekuma:/app/data
              ports: ["{{ ops_listen_ip }}:3001:3001"]
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "(which curl && curl -fsS http://localhost:3001/) || (which wget && wget -qO- http://localhost:3001/)"]
                interval: 10s
                timeout: 3s
                retries: 10
          volumes:
            prometheus: {}
            grafana: {}
            loki: {}
            portainer: {}
            registry: {}
            uptimekuma: {}
          secrets:
            grafana_admin:
              file: /srv/ops/config/secrets/grafana_admin
            portainer_admin:
              file: /srv/ops/config/secrets/portainer_admin

    - name: Installer: add-trust-ca.sh (trust SSH CA on targets)
      copy:
        dest: /srv/ops/installers/add-trust-ca.sh
        mode: "0755"
        content: |
          #!/usr/bin/env bash
          # Usage: add-trust-ca.sh <URL or /path/to/ssh_ca.pub> [revoked_keys]
          set -euo pipefail
          SRC="${1:-}"
          REV="${2:-}"
          TMP="$(mktemp)"
          if [[ -z "${SRC}" ]]; then
            echo "Usage: $0 <URL or /path/to/ssh_ca.pub> [revoked_keys]" >&2; exit 2
          fi
          if [[ "${SRC}" =~ ^https?:// ]]; then
            curl -fsSL "${SRC}" -o "${TMP}"
          else
            [[ -f "${SRC}" ]] || { echo "No such file: ${SRC}" >&2; exit 2; }
            cp -f "${SRC}" "${TMP}"
          fi
          sudo install -m0644 "${TMP}" /etc/ssh/trusted-user-ca-keys.pem
          rm -f "${TMP}"
          if [[ -n "${REV}" ]]; then
            TMP_REV="$(mktemp)"
            if [[ "${REV}" =~ ^https?:// ]]; then
              curl -fsSL "${REV}" -o "${TMP_REV}"
            else
              [[ -f "${REV}" ]] || { echo "No such file: ${REV}" >&2; exit 2; }
              cp -f "${REV}" "${TMP_REV}"
            fi
            sudo install -m0644 "${TMP_REV}" /etc/ssh/revoked_keys
            rm -f "${TMP_REV}"
            if grep -qE '^\s*RevokedKeys\b' /etc/ssh/sshd_config; then
              sudo sed -i 's#^\s*RevokedKeys.*#RevokedKeys /etc/ssh/revoked_keys#' /etc/ssh/sshd_config
            else
              echo 'RevokedKeys /etc/ssh/revoked_keys' | sudo tee -a /etc/ssh/sshd_config >/dev/null
            fi
          fi
          if grep -qE '^\s*TrustedUserCAKeys\b' /etc/ssh/sshd_config; then
            sudo sed -i 's#^\s*TrustedUserCAKeys.*#TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem#' /etc/ssh/sshd_config
          else
            echo 'TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem' | sudo tee -a /etc/ssh/sshd_config >/dev/null
          fi
          if systemctl list-unit-files | grep -q '^ssh\.service'; then
            sudo systemctl restart ssh
          else
            sudo systemctl restart sshd || true
          fi
          echo "Trusted SSH CA installed."

    - name: Generate installer: node_exporter + promtail
      template:
        src: templates/install-agents.sh.j2
        dest: /srv/ops/installers/install-agents.sh
        mode: "0755"

    - name: Systemd unit for ops compose
      copy:
        dest: /etc/systemd/system/ops-compose.service
        mode: "0644"
        content: |
          [Unit]
          Description=Ops Compose Stack
          After=docker.service network-online.target
          Requires=docker.service
          Wants=network-online.target
          [Service]
          Type=oneshot
          RemainAfterExit=yes
          WorkingDirectory=/srv/ops/compose
          ExecStart=/usr/bin/docker compose -f /srv/ops/compose/ops.yml up -d --wait
          ExecStop=/usr/bin/docker compose -f /srv/ops/compose/ops.yml down
          TimeoutStartSec=0
          Environment=DOCKER_CONFIG=/srv/ops/.docker
          ReadWritePaths=/srv/ops/.docker
          ProtectSystem=full
          ProtectHome=read-only
          PrivateTmp=true
          [Install]
          WantedBy=multi-user.target
      notify: reload systemd

    - name: Drop uninstaller helper
      copy:
        dest: /srv/ops/uninstall.sh
        mode: "0755"
        content: |
          #!/usr/bin/env bash
          set -euo pipefail
          systemctl stop ops-compose.service || true
          docker compose -f /srv/ops/compose/ops.yml down -v || true
          rm -f /etc/systemd/system/ops-compose.service
          systemctl daemon-reload || true
          echo "Stack removed. Config remains in /srv/ops."

    - name: Enable & start ops stack
      systemd:
        name: ops-compose.service
        enabled: true
        state: started

  handlers:
    - name: enable ufw
      ufw: { state: enabled }

    - name: restart fail2ban
      service: { name: fail2ban, state: restarted, enabled: true }

    - name: restart wireguard
      service: { name: wg-quick@wg0, state: restarted }

    - name: restart crowdsec
      service: { name: crowdsec, state: restarted }

    - name: reload systemd
      systemd: { daemon_reload: true }

    - name: restart docker
      service: { name: docker, state: restarted }

    - name: reload ssh
      service:
        name: "{{ 'sshd' if ('sshd.service' in ansible_facts.services) else 'ssh' }}"
        state: reloaded

    - name: restart node exporter
      service: { name: prometheus-node-exporter, state: restarted, enabled: true }
YAML
sed -i \
  -e "s/__PROMETHEUS_VERSION__/${PROMETHEUS_VERSION}/g" \
  -e "s/__ALERTMANAGER_VERSION__/${ALERTMANAGER_VERSION}/g" \
  -e "s/__GRAFANA_VERSION__/${GRAFANA_VERSION}/g" \
  -e "s/__LOKI_VERSION__/${LOKI_VERSION}/g" \
  -e "s/__PROMTAIL_VERSION__/${PROMTAIL_VERSION}/g" \
  -e "s/__PORTAINER_VERSION__/${PORTAINER_VERSION}/g" \
  -e "s/__REGISTRY_VERSION__/${REGISTRY_VERSION}/g" \
  -e "s/__UPTIME_KUMA_VERSION__/${UPTIME_KUMA_VERSION}/g" \
  site.yml

# ------------------------------------------------------------
# Run the playbook
# ------------------------------------------------------------
if [ -z "${HEALTH_ONLY:-}" ]; then
  say "Running Ansible (local)…"
  if [ -n "${DRY_RUN:-}" ]; then
    ansible-playbook -i inventory.ini site.yml --check --diff
  else
    ansible-playbook -i inventory.ini site.yml
  fi
fi

# ------------------------------------------------------------
# Service health checks
# ------------------------------------------------------------
if [ -z "${DRY_RUN:-}" ]; then
  say "Checking services…"
  fail=0
  [ -d /srv/ops/compose ] || nope "No stack at /srv/ops/compose. Run 'run' first."
  cd /srv/ops/compose || nope "Cannot cd to compose dir"
  docker compose -f ops.yml ps || fail=1

  probe(){ # name url
    local name="$1" url="$2" retries=3 i
    for i in $(seq "$retries"); do
      if curl -fsS --connect-timeout 5 --max-time 10 "$url" >/dev/null 2>&1; then
        say "$name responding at $url"
        return 0
      fi
      sleep 3
    done
    say "$name FAILED at $url"
    fail=1
  }

  probe "Grafana"      "http://localhost:3000/login"
  probe "Prometheus"   "http://localhost:9090/-/ready"
  probe "Alertmanager" "http://localhost:9093/-/ready"
  probe "Loki"         "http://localhost:3100/ready"
  probe "Portainer"    "http://localhost:9000"
  probe "Registry"     "http://localhost:5000/v2/"
  probe "Uptime Kuma"  "http://localhost:3001"

  if [ "$fail" -ne 0 ]; then
    nope "Service checks failed. Use 'docker compose -f /srv/ops/compose/ops.yml ps' and inspect logs."
  fi
fi

# Quick stack info
docker --version | tee -a "$LOG_FILE"
ansible --version | head -n1 | tee -a "$LOG_FILE"
printf "[+] Images pinned: prom %s, alert %s, grafana %s, loki %s, promtail %s\n" \
  "$PROMETHEUS_VERSION" "$ALERTMANAGER_VERSION" "$GRAFANA_VERSION" "$LOKI_VERSION" "$PROMTAIL_VERSION" \
  | ${SUDO:-} tee -a "$LOG_FILE" >/dev/null

# ------------------------------------------------------------
# Exit banner
# ------------------------------------------------------------
OPS_IP=$(awk -F': ' '/^ops_listen_ip:/ {print $2}' /srv/ops/ansible/group_vars/all.yml | tr -d '"')
IP_NOW=${OPS_IP:-$(hostname -I | awk '{print $1}')}
say "Done. If you were added to the docker group, log out/in. URLs:"
say " Grafana       : http://$IP_NOW:3000 (admin / see group_vars/all.yml: grafana_admin_password)"
say " Prometheus    : http://$IP_NOW:9090"
say " Alertmanager  : http://$IP_NOW:9093"
say " Loki (API)    : http://$IP_NOW:3100"
say " Portainer     : http://$IP_NOW:9000 (admin / password printed during install)"
say " Registry      : http://localhost:5000 (use docker login)"
say " Uptime Kuma   : http://$IP_NOW:3001"
say " Log file      : $LOG_FILE"
