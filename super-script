#!/usr/bin/env bash
# ============================================================================
# SUPERSETUP v2 — first Ops box bootstrap (Ubuntu/Debian)
# Fast track: Docker, observability (Prom/Grafana/Loki/Alertmanager), Portainer,
# local registry, SSH CA, Uptime-Kuma, and an Ansible project to manage it.
# Sarcastic comments, minimal ceremony, actually idempotent.
# ============================================================================

set -euo pipefail
trap 'nope "Failed at line $LINENO. See $LOG_FILE for details."' ERR
IFS=$'\n\t'
LOG_FILE=/var/log/supersetup.log # standard location for easy rotation

say(){
  printf "\033[1;32m[+]\033[0m %s\n" "$*"
  printf "[+] %s\n" "$*" | ${SUDO:-} tee -a "$LOG_FILE" >/dev/null
}
nope(){
  printf "\033[1;31m[!]\033[0m %s\n" "$*" >&2
  printf "[!] %s\n" "$*" | ${SUDO:-} tee -a "$LOG_FILE" >/dev/null
  exit 1
}

# ------------------------------------------------------------
# Version variables (override via environment or versions file)
# ------------------------------------------------------------
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VERSION_FILE="${VERSION_FILE:-$SCRIPT_DIR/versions.env}"
# shellcheck source=/dev/null
[ -f "$VERSION_FILE" ] && . "$VERSION_FILE"

# Provide sane defaults; environment or versions file can override
TERRAFORM_VERSION="${TERRAFORM_VERSION:-1.9.5}"
PACKER_VERSION="${PACKER_VERSION:-1.11.2}"
TRIVY_VERSION="${TRIVY_VERSION:-0.54.1}"
PROMETHEUS_VERSION="${PROMETHEUS_VERSION:-v2.55.0}"
ALERTMANAGER_VERSION="${ALERTMANAGER_VERSION:-v0.27.0}"
GRAFANA_VERSION="${GRAFANA_VERSION:-10.4.6}"
LOKI_VERSION="${LOKI_VERSION:-2.9.8}"
PROMTAIL_VERSION="${PROMTAIL_VERSION:-2.9.8}"
PORTAINER_VERSION="${PORTAINER_VERSION:-2.21.4}"
REGISTRY_VERSION="${REGISTRY_VERSION:-2}"
UPTIME_KUMA_VERSION="${UPTIME_KUMA_VERSION:-1}"

# ------------------------------------------------------------
# CLI modes: run (default), dry-run, health
# ------------------------------------------------------------
MODE="${1:-run}"   # run | dry-run | health
case "$MODE" in
  run) : ;;
  dry-run) export DRY_RUN=1 ;;
  health) export HEALTH_ONLY=1 ;;
  *) nope "Unknown mode: $MODE (use: run|dry-run|health)";;
esac

# ------------------------------------------------------------
# Sanity checks
# ------------------------------------------------------------
[ "$(id -u)" -eq 0 ] && SUDO="" || SUDO="sudo"
ME="${OPS_OWNER:-${SUDO_USER:-${LOGNAME:-$USER}}}"
say "Ops owner user: ${ME}"
if [ -n "$SUDO" ] && ! sudo -n true 2>/dev/null; then
  nope "Passwordless sudo required for non-root runs. Configure NOPASSWD or run the script with sudo."
fi
$SUDO touch "$LOG_FILE"
$SUDO chown root:root "$LOG_FILE"
$SUDO chmod 0644 "$LOG_FILE"
. /etc/os-release || nope "Unknown OS."
if [[ ! "${ID}" =~ (ubuntu|debian) ]] && [[ ! "${ID_LIKE:-}" =~ (ubuntu|debian) ]]; then
  nope "Use Ubuntu/Debian for this bootstrap."
fi

# Retry wrapper for apt-get commands
apt_retry(){ # args: apt-get subcommand + options
  local n=0; until [ $n -ge 3 ]; do
    $SUDO apt-get "$@" && break
    n=$((n+1)); sleep 3
    say "apt-get retry $n…"
  done
  [ $n -lt 3 ] || nope "apt-get failed after retries: $*"
}

# ------------------------------------------------------------
# Base packages
# ------------------------------------------------------------
say "Updating packages…"
apt_retry update -y
apt_retry install -y --no-install-recommends \
  ca-certificates gnupg lsb-release curl git unzip tar make jq \
  ufw fail2ban python3 python3-pip nmap

# ------------------------------------------------------------
# Docker CE (official repo)
# ------------------------------------------------------------
install_docker(){
  if command -v docker >/dev/null 2>&1; then
    say "Docker already present. Enabling service."
    $SUDO systemctl enable --now docker || true
    return 0
  fi
  say "Installing Docker CE…"
  # Remove conflicting packages quietly
  $SUDO apt-get -y remove docker.io containerd runc || true
  $SUDO apt-get -y purge  docker.io containerd runc || true

  $SUDO install -m 0755 -d /etc/apt/keyrings
  [ -f /etc/apt/keyrings/docker.gpg ] || \
    curl -fsSL "https://download.docker.com/linux/${ID}/gpg" | $SUDO gpg --dearmor -o /etc/apt/keyrings/docker.gpg

  if [ ! -f /etc/apt/sources.list.d/docker.list ]; then
    arch=$(dpkg --print-architecture)
    codename=$(. /etc/os-release && echo "$VERSION_CODENAME")
    echo "deb [arch=${arch} signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/${ID} ${codename} stable" | \
      $SUDO tee /etc/apt/sources.list.d/docker.list >/dev/null
  fi
  apt_retry update -y
  apt_retry install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  $SUDO systemctl enable --now docker
}
install_docker

$SUDO usermod -aG docker "$ME" || true

# ------------------------------------------------------------
# Helpers to install single-file archives
# ------------------------------------------------------------
install_zip_bin(){ # name ver url sha256
  local name="$1" ver="$2" url="$3" sha="$4"
  command -v "$name" >/dev/null 2>&1 && return 0
  say "Installing $name $ver…"
  local tmp
  tmp=$(mktemp "/tmp/${name}.XXXXXX.zip")
  curl -fsSL -o "$tmp" "$url"
  local sum
  sum=$(sha256sum "$tmp" | awk '{print $1}')
  if [ "$sum" != "$sha" ]; then
    rm -f "$tmp"
    nope "$name checksum mismatch"
  fi
  $SUDO unzip -o "$tmp" -d /usr/local/bin/
  $SUDO chmod +x "/usr/local/bin/${name}"
  rm -f "$tmp"
}

install_tgz_bin(){ # name url sha256
  local name="$1" url="$2" sha="$3"
  command -v "$name" >/dev/null 2>&1 && return 0
  say "Installing $name…"
  local tmp tmpdir sum bin
  tmp=$(mktemp "/tmp/${name}.XXXXXX.tgz")
  curl -fsSL -o "$tmp" "$url"
  sum=$(sha256sum "$tmp" | awk '{print $1}')
  if [ "$sum" != "$sha" ]; then
    rm -f "$tmp"
    nope "$name checksum mismatch"
  fi
  tmpdir=$(mktemp -d "/tmp/${name}-XXXXXX")
  tar -xzf "$tmp" -C "$tmpdir"
  bin=$(find "$tmpdir" -type f -name "$name" | head -n1 || true)
  if [ -z "$bin" ]; then
    rm -f "$tmp"
    rm -rf "$tmpdir"
    nope "Failed to locate $name in archive"
  fi
  $SUDO install -m0755 "$bin" "/usr/local/bin/${name}"
  rm -f "$tmp"
  rm -rf "$tmpdir"
}

# Arch detect
ARCH="$(uname -m)"
case "$ARCH" in
  x86_64) ARCH=amd64 ;;
  aarch64|arm64) ARCH=arm64 ;;
  armv7l) ARCH=arm ;;
  *) say "Unknown arch $ARCH; URLs may need adjusting." ;;
esac

# Terraform
tf_url="https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_${ARCH}.zip"
tf_sha="$(curl -fsSL "https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_SHA256SUMS" \
  | grep "terraform_${TERRAFORM_VERSION}_linux_${ARCH}.zip" | awk '{print $1}')"
install_zip_bin terraform "$TERRAFORM_VERSION" "$tf_url" "$tf_sha"

# Packer
packer_url="https://releases.hashicorp.com/packer/${PACKER_VERSION}/packer_${PACKER_VERSION}_linux_${ARCH}.zip"
packer_sha="$(curl -fsSL "https://releases.hashicorp.com/packer/${PACKER_VERSION}/packer_${PACKER_VERSION}_SHA256SUMS" \
  | grep "packer_${PACKER_VERSION}_linux_${ARCH}.zip" | awk '{print $1}')"
install_zip_bin packer "$PACKER_VERSION" "$packer_url" "$packer_sha"

# Trivy (prebuilt for amd64 and arm64)
if [ "$ARCH" = "amd64" ]; then
  trivy_url="https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_Linux-64bit.tar.gz"
  trivy_sha="$(curl -fsSL "https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_checksums.txt" \
    | grep "trivy_${TRIVY_VERSION}_Linux-64bit.tar.gz" | awk '{print $1}')"
  install_tgz_bin trivy "$trivy_url" "$trivy_sha" || true
elif [ "$ARCH" = "arm64" ]; then
  trivy_url="https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_Linux-ARM64.tar.gz"
  trivy_sha="$(curl -fsSL "https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_checksums.txt" \
    | grep "trivy_${TRIVY_VERSION}_Linux-ARM64.tar.gz" | awk '{print $1}')"
  install_tgz_bin trivy "$trivy_url" "$trivy_sha" || true
else
  say "Skipping trivy prebuilt for ARCH=$ARCH; install manually later."
fi

# ------------------------------------------------------------
# Ansible
# ------------------------------------------------------------
say "Installing Ansible…"
apt_retry install -y ansible

# ------------------------------------------------------------
# Scaffold the ops project — inventories, vars, templates, playbook, compose
# ------------------------------------------------------------
say "Laying down /srv/ops/ansible…"
$SUDO mkdir -p /srv/ops/ansible/templates
# ownership fix: use $ME (not ${USER} which can be root)
$SUDO chown -R "$ME":"$ME" /srv/ops
cd /srv/ops/ansible

# -------- inventory --------
cat > inventory.ini <<'INI'
[ops]
localhost ansible_connection=local
INI

# -------- group vars --------
mkdir -p group_vars
cat > group_vars/all.yml <<'YAML'
# Optional front-door hostnames (for proxy/TLS later)
ops_domain: ""  # e.g., ops.example.com
ops_listen_ip: "0.0.0.0"        # change to your LAN IP to limit exposure
slack_webhook_url: ""           # set for Alertmanager
telegram_bot_token: ""
telegram_chat_id: ""

enable_wireguard: false     # Set to true to install WireGuard VPN
enable_crowdsec: false      # Set to true to install CrowdSec IDS

# WireGuard defaults (adjust to your environment)
wireguard_address: "10.0.0.1/24"
wireguard_listen_port: 51820
wireguard_private_key: "<changeme>"
wireguard_peers: []

# CrowdSec log sources
crowdsec_log_files:
  - /var/log/auth.log

grafana_admin_password: "changeme"  # change post-install or set here

# Multinode-curious knobs (safe defaults for single host)
prom_host: "localhost"
loki_host: "localhost"
grafana_host: "localhost"

prometheus_url: "http://{{ prom_host }}:9090"
loki_url:       "http://{{ loki_host }}:3100"

# Prometheus targets (expand as you onboard servers)
prom_targets: []  # Add more targets like "10.0.0.12:9100", "web-1:9100" etc. The local host is auto-added.
YAML

# -------- templates --------
# Prometheus
cat > templates/prometheus.yml.j2 <<'J2'
global:
  scrape_interval: 15s
rule_files:
  - /etc/prometheus/alert_rules.yml
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  - job_name: 'nodes'
    static_configs:
      - targets: {{ ((prom_targets | default([])) + [ansible_default_ipv4.address ~ ':9100']) | unique | to_json }}
J2

# Alertmanager (Slack optional)
cat > templates/alertmanager.yml.j2 <<'J2'
route:
  receiver: {{ 'slack' if slack_webhook_url|length > 0 else 'default' }}
  group_by: ['alertname']
  group_wait: 30s
  repeat_interval: 3h
receivers:
  - name: default
{% if slack_webhook_url|length > 0 %}
  - name: slack
    slack_configs:
      - api_url: "{{ slack_webhook_url }}"
        send_resolved: true
        channel: "#alerts"
        title: |
          {{ '{{' }} .CommonAnnotations.summary {{ '}}' }}
        text: |
          {{ '{{' }} range .Alerts {{ '}}' }}*{{ '{{' }} .Annotations.summary {{ '}}' }}* on {{ '{{' }} .Labels.instance {{ '}}' }}{{ '{{' }} end {{ '}}' }}
{% endif %}
J2

# Promtail (points to this box's Loki)
cat > templates/promtail-config.yml.j2 <<'J2'
server:
  http_listen_port: 9080
clients:
  - url: "{{ loki_url }}/loki/api/v1/push"
positions:
  filename: /var/lib/promtail/positions.yaml
scrape_configs:
  - job_name: system
    static_configs:
      - targets: [localhost]
        labels:
          job: varlogs
          __path__: /var/log/**/*.log
J2

# Agent installer (for remote hosts)
cat > templates/install-agents.sh.j2 <<'J2'
#!/usr/bin/env bash
set -euo pipefail
if command -v apt-get >/dev/null; then
  apt_retry(){ # args: apt-get subcommand + options
    local n=0; until [ $n -ge 3 ]; do
      sudo apt-get "$@" && break
      n=$((n+1)); sleep 3
      echo "apt-get retry $n…" >&2
    done
    [ $n -lt 3 ]
  }
  apt_retry update -y
  apt_retry install -y prometheus-node-exporter
  sudo systemctl enable --now prometheus-node-exporter
fi
sudo mkdir -p /etc/promtail /var/lib/promtail
cat <<CFG | sudo tee /etc/promtail/config.yml >/dev/null
server: { http_listen_port: 9080 }
clients: [ { url: "http://{{ ansible_default_ipv4.address }}:3100/loki/api/v1/push" } ]
positions: { filename: /var/lib/promtail/positions.yaml }
scrape_configs:
  - job_name: varlogs
    static_configs:
      - targets: [localhost]
        labels:
          job: varlogs
          __path__: /var/log/**/*.log
CFG
# Run promtail via Docker for simplicity
sudo docker run -d --name promtail --restart unless-stopped \
  -v /etc/promtail/config.yml:/etc/promtail/config.yml:ro \
  -v /var/log:/var/log:ro \
  -v /var/lib/promtail:/var/lib/promtail \
  -p 9080:9080 \
  grafana/promtail:__PROMTAIL_VERSION__ -config.file=/etc/promtail/config.yml
J2
sed -i "s/__PROMTAIL_VERSION__/${PROMTAIL_VERSION}/g" templates/install-agents.sh.j2

# WireGuard interface config
cat > templates/wg0.conf.j2 <<'J2'
[Interface]
Address = {{ wireguard_address }}
ListenPort = {{ wireguard_listen_port }}
PrivateKey = {{ wireguard_private_key }}
{% for peer in wireguard_peers %}
[Peer]
PublicKey = {{ peer.public_key }}
AllowedIPs = {{ peer.allowed_ips }}
{% if peer.endpoint is defined %}Endpoint = {{ peer.endpoint }}{% endif %}
{% endfor %}
J2

# CrowdSec acquisition config
cat > templates/crowdsec-acquis.yaml.j2 <<'J2'
filenames:
{% for file in crowdsec_log_files %}
  - {{ file }}
{% endfor %}
J2

# -------- site playbook --------
cat > site.yml <<'YAML'
---
- hosts: ops
  become: true
  vars_files:
    - group_vars/all.yml

  pre_tasks:
    - name: Gather facts
      setup:
    - name: Collect service facts
      service_facts:

  tasks:
    - name: Create ops directories
      file:
        path: "{{ item }}"
        state: directory
        recurse: yes
        mode: "0755"
      loop:
        - /srv/ops/compose
        - /srv/ops/config
        - /srv/ops/installers
        - /srv/ops/data/prometheus
        - /srv/ops/data/grafana
        - /srv/ops/data/loki
        - /srv/ops/data/registry
        - /srv/ops/data/uptime-kuma
        - /opt/ssh-ca

    - name: Generate OpenSSH CA (once)
      command: ssh-keygen -t ed25519 -f /opt/ssh-ca/ssh_ca -N "" -C "ops-ssh-ca"
      args: { creates: /opt/ssh-ca/ssh_ca }

    - name: Publish TrustedUserCAKeys
      copy:
        src: /opt/ssh-ca/ssh_ca.pub
        dest: /etc/ssh/trusted-user-ca-keys.pem
        owner: root
        group: root
        mode: "0644"

    - name: Enforce CA trust in sshd
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^TrustedUserCAKeys'
        line: 'TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem'
        create: yes
        backup: yes

    - name: Reload SSH
      service:
        name: "{{ 'sshd' if ('sshd.service' in ansible_facts.services) else 'ssh' }}"
        state: reloaded

    - name: Signer helper
      copy:
        dest: /usr/local/bin/ssh-sign
        mode: "0755"
        content: |
          #!/usr/bin/env bash
          # ssh-sign <user_pubkey_file> <principals> <hours_valid>
          set -euo pipefail
          PUB="${1:-}"; PRIN="${2:-$USER}"; HRS="${3:-24}"
          [[ -f "$PUB" ]] || { echo "Need a public key file"; exit 1; }
          ssh-keygen -s /opt/ssh-ca/ssh_ca -I "$PRIN" -n "$PRIN" -V +"${HRS}"h "$PUB"
          echo "Signed cert: ${PUB}-cert.pub (valid ${HRS}h for ${PRIN})"

    # -------- UFW: safe sequence --------
    - name: UFW allow SSH before enabling
      ufw:
        rule: allow
        port: "22"

    - name: Set UFW default incoming policy to deny
      ufw:
        state: disabled
        direction: incoming
        policy: deny
    - name: Set UFW default outgoing policy to allow
      ufw:
        state: disabled
        direction: outgoing
        policy: allow

    - name: Allow required ports
      ufw:
        rule: allow
        port: "{{ item }}"
      loop:
        - "22"    # SSH
        - "3000"  # Grafana
        - "9090"  # Prometheus
        - "9093"  # Alertmanager
        - "3100"  # Loki
        - "9000"  # Portainer
        - "3001"  # Uptime Kuma
      notify: enable ufw

    - name: Allow WireGuard port
      ufw:
        rule: allow
        port: "{{ wireguard_listen_port }}"
        proto: udp
      when: enable_wireguard

    - name: Fail2ban config (basic ssh)
      copy:
        dest: /etc/fail2ban/jail.d/ssh.local
        content: |
          [sshd]
          enabled = true
          maxretry = 6
          bantime = 1h
          findtime = 10m
      notify: restart fail2ban

    # -------- Optional WireGuard VPN --------
    - name: Install WireGuard
      apt:
        name: wireguard
        state: present
        update_cache: yes
      when: enable_wireguard

    - name: Validate WireGuard key placeholder
      fail:
        msg: "Set a real wireguard_private_key in group_vars/all.yml"
      when: enable_wireguard and (wireguard_private_key is match('^<changeme>'))

    - name: WireGuard config
      template:
        src: templates/wg0.conf.j2
        dest: /etc/wireguard/wg0.conf
        mode: "0600"
      when: enable_wireguard
      notify: restart wireguard

    - name: Enable WireGuard service
      systemd:
        name: wg-quick@wg0
        enabled: true
        state: started
      when: enable_wireguard

    # -------- Optional CrowdSec IDS --------
    - name: Install CrowdSec
      apt:
        name:
          - crowdsec
          - crowdsec-firewall-bouncer-iptables
        state: present
        update_cache: yes
      when: enable_crowdsec

    - name: CrowdSec acquisition config
      template:
        src: templates/crowdsec-acquis.yaml.j2
        dest: /etc/crowdsec/acquis.yaml
        mode: "0644"
      when: enable_crowdsec
      notify: restart crowdsec

    - name: Ensure CrowdSec service
      service: { name: crowdsec, state: started, enabled: true }
      when: enable_crowdsec

    - name: Ensure CrowdSec firewall bouncer
      service: { name: crowdsec-firewall-bouncer, state: started, enabled: true }
      when: enable_crowdsec

    # -------- Docker daemon.json --------
    - name: Configure Docker log rotation + BuildKit
      copy:
        dest: /etc/docker/daemon.json
        mode: "0644"
        content: |
          {
            "log-driver": "json-file",
            "log-opts": { "max-size": "10m", "max-file": "5" },
            "features": { "buildkit": true }
          }
      notify: restart docker

    - name: Ensure node exporter installed
      apt:
        name: prometheus-node-exporter
        state: present
        update_cache: yes

    - name: Enable node exporter service
      service: { name: prometheus-node-exporter, state: started, enabled: true }

    - name: Prometheus config
      template:
        src: templates/prometheus.yml.j2
        dest: /srv/ops/config/prometheus.yml
        mode: "0644"

    - name: Alertmanager config
      template:
        src: templates/alertmanager.yml.j2
        dest: /srv/ops/config/alertmanager.yml
        mode: "0644"

    # -------- Prometheus alert rules file (bind mount target) --------
    - name: Ensure Prometheus alert rules file exists
      copy:
        dest: /srv/ops/config/alert_rules.yml
        mode: "0644"
        content: |
          groups:
            - name: basic-host
              rules:
                - alert: HostDown
                  expr: up == 0
                  for: 2m
                  labels:
                    severity: critical
                  annotations:
                    summary: "Instance {{ $labels.instance }} is down"

    - name: Loki config
      copy:
        dest: /srv/ops/config/loki-config.yml
        mode: "0644"
        content: |
          auth_enabled: false
          server: { http_listen_port: 3100 }
          common:
            path_prefix: /loki
          storage_config:
            filesystem: { }
          schema_config:
            configs:
              - from: 2020-10-24
                store: boltdb-shipper
                object_store: filesystem
                schema: v11
                index:
                  prefix: index_
                  period: 24h

    # -------- Promtail state dir --------
    - name: Ensure promtail state dir exists
      file:
        path: /var/lib/promtail
        state: directory
        mode: "0755"

    - name: Promtail config (this host)
      template:
        src: templates/promtail-config.yml.j2
        dest: /srv/ops/config/promtail-config.yml
        mode: "0644"

    # -------- Grafana provisioning (no click-ops) --------
    - name: Grafana provisioning dirs
      file:
        path: /srv/ops/config/grafana/provisioning/datasources
        state: directory
        mode: "0755"

    - name: Grafana default datasources
      copy:
        dest: /srv/ops/config/grafana/provisioning/datasources/datasources.yml
        mode: "0644"
        content: |
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://prometheus:9090
              isDefault: true
            - name: Loki
              type: loki
              access: proxy
              url: http://loki:3100

    - name: Compose file for ops stack
      copy:
        dest: /srv/ops/compose/ops.yml
        mode: "0644"
        content: |
          services:
            # For internet exposure: put behind a reverse proxy (Traefik/Nginx) with TLS.
            # These ports are LAN-facing convenience only.
            prometheus:
              image: prom/prometheus:__PROMETHEUS_VERSION__
              command: ["--config.file=/etc/prometheus/prometheus.yml","--web.enable-lifecycle","--web.route-prefix=/"]
              volumes:
                - /srv/ops/config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
                - /srv/ops/config/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
                - prometheus:/prometheus
                # Bind-mount alternative (pick one, then remove named volume "prometheus" below):
                # - /srv/ops/data/prometheus:/prometheus
              ports: ["9090:9090"]
              restart: unless-stopped
            alertmanager:
              image: prom/alertmanager:__ALERTMANAGER_VERSION__
              volumes:
                - /srv/ops/config/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
              ports: ["9093:9093"]
              restart: unless-stopped
            grafana:
              image: grafana/grafana:__GRAFANA_VERSION__
              environment:
                - GF_SECURITY_ADMIN_PASSWORD={{ grafana_admin_password }}
              volumes:
                - grafana:/var/lib/grafana
                - /srv/ops/config/grafana/provisioning:/etc/grafana/provisioning:ro
              ports: ["3000:3000"]
              restart: unless-stopped
            loki:
              image: grafana/loki:__LOKI_VERSION__
              command: ["-config.file=/etc/loki/loki-config.yml"]
              volumes:
                - /srv/ops/config/loki-config.yml:/etc/loki/loki-config.yml:ro
                - loki:/loki
              ports: ["3100:3100"]
              restart: unless-stopped
            promtail:
              image: grafana/promtail:__PROMTAIL_VERSION__
              command: ["-config.file=/etc/promtail/config.yml"]
              volumes:
                - /srv/ops/config/promtail-config.yml:/etc/promtail/config.yml:ro
                - /var/log:/var/log:ro
                - /var/lib/promtail:/var/lib/promtail
              restart: unless-stopped
            portainer:
              image: portainer/portainer-ce:__PORTAINER_VERSION__
              volumes:
                - /var/run/docker.sock:/var/run/docker.sock
                - portainer:/data
              ports: ["9000:9000"]
              restart: unless-stopped
            registry:
              image: registry:__REGISTRY_VERSION__
              environment:
                REGISTRY_STORAGE_DELETE_ENABLED: "true"
              volumes:
                - registry:/var/lib/registry
              ports: ["127.0.0.1:5000:5000"]
              restart: unless-stopped
            uptime-kuma:
              image: louislam/uptime-kuma:__UPTIME_KUMA_VERSION__
              volumes:
                - uptimekuma:/app/data
              ports: ["3001:3001"]
              restart: unless-stopped
          volumes:
            prometheus: {}
            grafana: {}
            loki: {}
            portainer: {}
            registry: {}
            uptimekuma: {}

    - name: Installer: add-trust-ca.sh (trust SSH CA on targets)
      copy:
        dest: /srv/ops/installers/add-trust-ca.sh
        mode: "0755"
        content: |
          #!/usr/bin/env bash
          # Usage: add-trust-ca.sh <URL or /path/to/ssh_ca.pub>
          set -euo pipefail
          SRC="${1:-}"
          TMP="$(mktemp)"
          if [[ -z "${SRC}" ]]; then
            echo "Usage: $0 <URL or /path/to/ssh_ca.pub>" >&2; exit 2
          fi
          if [[ "${SRC}" =~ ^https?:// ]]; then
            curl -fsSL "${SRC}" -o "${TMP}"
          else
            [[ -f "${SRC}" ]] || { echo "No such file: ${SRC}" >&2; exit 2; }
            cp -f "${SRC}" "${TMP}"
          fi
          sudo install -m0644 "${TMP}" /etc/ssh/trusted-user-ca-keys.pem
          rm -f "${TMP}"
          if grep -qE '^\s*TrustedUserCAKeys\b' /etc/ssh/sshd_config; then
            sudo sed -i 's#^\s*TrustedUserCAKeys.*#TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem#' /etc/ssh/sshd_config
          else
            echo 'TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem' | sudo tee -a /etc/ssh/sshd_config >/dev/null
          fi
          if systemctl list-unit-files | grep -q '^ssh\.service'; then
            sudo systemctl restart ssh
          else
            sudo systemctl restart sshd || true
          fi
          echo "Trusted SSH CA installed."

    - name: Generate installer: node_exporter + promtail
      template:
        src: templates/install-agents.sh.j2
        dest: /srv/ops/installers/install-agents.sh
        mode: "0755"

    - name: Systemd unit for ops compose
      copy:
        dest: /etc/systemd/system/ops-compose.service
        mode: "0644"
        content: |
          [Unit]
          Description=Ops Compose Stack
          After=docker.service
          Requires=docker.service
          [Service]
          Type=oneshot
          RemainAfterExit=yes
          WorkingDirectory=/srv/ops/compose
          ExecStart=/usr/bin/docker compose -f /srv/ops/compose/ops.yml up -d
          ExecStop=/usr/bin/docker compose -f /srv/ops/compose/ops.yml down
          [Install]
          WantedBy=multi-user.target
      notify: reload systemd

    - name: Drop uninstaller helper
      copy:
        dest: /srv/ops/uninstall.sh
        mode: "0755"
        content: |
          #!/usr/bin/env bash
          set -euo pipefail
          systemctl stop ops-compose.service || true
          docker compose -f /srv/ops/compose/ops.yml down -v || true
          rm -f /etc/systemd/system/ops-compose.service
          systemctl daemon-reload || true
          echo "Stack removed. Config remains in /srv/ops."

    - name: Enable & start ops stack
      systemd:
        name: ops-compose.service
        enabled: true
        state: started

  handlers:
    - name: enable ufw
      ufw: { state: enabled }

    - name: restart fail2ban
      service: { name: fail2ban, state: restarted, enabled: true }

    - name: restart wireguard
      service: { name: wg-quick@wg0, state: restarted }

    - name: restart crowdsec
      service: { name: crowdsec, state: restarted }

    - name: reload systemd
      systemd: { daemon_reload: true }

    - name: restart docker
      service: { name: docker, state: restarted }
YAML
sed -i \
  -e "s/__PROMETHEUS_VERSION__/${PROMETHEUS_VERSION}/g" \
  -e "s/__ALERTMANAGER_VERSION__/${ALERTMANAGER_VERSION}/g" \
  -e "s/__GRAFANA_VERSION__/${GRAFANA_VERSION}/g" \
  -e "s/__LOKI_VERSION__/${LOKI_VERSION}/g" \
  -e "s/__PROMTAIL_VERSION__/${PROMTAIL_VERSION}/g" \
  -e "s/__PORTAINER_VERSION__/${PORTAINER_VERSION}/g" \
  -e "s/__REGISTRY_VERSION__/${REGISTRY_VERSION}/g" \
  -e "s/__UPTIME_KUMA_VERSION__/${UPTIME_KUMA_VERSION}/g" \
  site.yml

# ------------------------------------------------------------
# Run the playbook
# ------------------------------------------------------------
if [ -z "${HEALTH_ONLY:-}" ]; then
  say "Running Ansible (local)…"
  if [ -n "${DRY_RUN:-}" ]; then
    ansible-playbook -i inventory.ini site.yml --check --diff
  else
    ansible-playbook -i inventory.ini site.yml
  fi
fi

# ------------------------------------------------------------
# Service health checks
# ------------------------------------------------------------
if [ -z "${DRY_RUN:-}" ]; then
  say "Checking services…"
  fail=0
  cd /srv/ops/compose
  docker compose -f ops.yml ps || fail=1

  probe(){ # name url
    local name="$1" url="$2" retries=3 i
    for i in $(seq "$retries"); do
      if curl -fsS --connect-timeout 5 --max-time 10 "$url" >/dev/null 2>&1; then
        say "$name responding at $url"
        return 0
      fi
      sleep 3
    done
    say "$name FAILED at $url"
    fail=1
  }

  probe "Grafana"      "http://localhost:3000/login"
  probe "Prometheus"   "http://localhost:9090/-/ready"
  probe "Alertmanager" "http://localhost:9093/-/ready"
  probe "Loki"         "http://localhost:3100/ready"
  probe "Portainer"    "http://localhost:9000"
  probe "Registry"     "http://localhost:5000/v2/"
  probe "Uptime Kuma"  "http://localhost:3001"

  if [ "$fail" -ne 0 ]; then
    nope "Service checks failed. Use 'docker compose -f /srv/ops/compose/ops.yml ps' and inspect logs."
  fi
fi

# ------------------------------------------------------------
# Exit banner
# ------------------------------------------------------------
if grep -q 'grafana_admin_password: "changeme"' /srv/ops/ansible/group_vars/all.yml 2>/dev/null; then
  say "WARNING: Grafana admin password is still 'changeme'; update /srv/ops/ansible/group_vars/all.yml"
fi

IP_NOW=$(hostname -I | awk '{print $1}')
say "Done. If you were added to the docker group, log out/in. URLs:"
say " Grafana       : http://$IP_NOW:3000 (admin / see group_vars/all.yml: grafana_admin_password)"
say " Prometheus    : http://$IP_NOW:9090"
say " Alertmanager  : http://$IP_NOW:9093"
say " Loki (API)    : http://$IP_NOW:3100"
say " Portainer     : http://$IP_NOW:9000"
say " Registry      : http://localhost:5000 (use docker login)"
say " Uptime Kuma   : http://$IP_NOW:3001"
say " Log file      : $LOG_FILE"
